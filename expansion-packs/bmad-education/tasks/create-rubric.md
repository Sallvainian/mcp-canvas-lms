# Create Rubric Task

## Purpose
Design effective rubrics (analytic and holistic) that provide clear performance criteria, consistent scoring, and actionable feedback for complex student work, supporting both assessment and learning.

## Prerequisites
- Learning objectives defined (see `create-learning-objectives.md`)
- Assessment task designed (see `create-assessment.md`)
- Understanding of rubric types and components
- Access to Canvas MCP tools
- Examples of student work (if available)

## Step-by-Step Process

### Step 1: Determine Rubric Type

#### 1.1 Choose Rubric Format
**Holistic Rubric** (single overall score):
- **Use When**:
  - Overall quality or impression most important
  - Quick scoring needed
  - General proficiency level sufficient
- **Advantages**: Fast, simple, big-picture view
- **Limitations**: Less diagnostic, limited feedback
- **Example Use Cases**: Essay drafts, presentations, overall project quality

**Analytic Rubric** (separate scores for multiple criteria):
- **Use When**:
  - Detailed feedback needed
  - Multiple distinct dimensions of quality
  - Diagnostic information valuable
  - Teaching tool for students
- **Advantages**: Specific feedback, identifies strengths/weaknesses, instructional value
- **Limitations**: More time-consuming to create and use
- **Example Use Cases**: Research papers, complex projects, skill demonstrations

**Single-Point Rubric** (describes proficiency, spaces for above/below):
- **Use When**:
  - Focus on standards/proficiency
  - Personalized feedback important
  - Growth-oriented assessment
- **Advantages**: Flexible, student-centered, focuses on meeting standards
- **Limitations**: Less prescriptive, requires more narrative feedback

**Decision Criteria**:
- **Complexity of task**: Simple → Holistic; Complex → Analytic
- **Feedback needs**: Summative grade → Holistic; Formative guidance → Analytic
- **Time available**: Limited → Holistic; Ample → Analytic
- **Primary purpose**: Evaluation → Holistic; Learning → Analytic

#### 1.2 Determine Number of Performance Levels
**Common Scales**:
- **3-Point**: Beginning, Developing, Proficient
- **4-Point**: Beginning, Developing, Proficient, Advanced/Exemplary
- **5-Point**: Below Basic, Basic, Proficient, Advanced, Exemplary
- **6-Point**: More granularity (use cautiously)

**Guidelines**:
- **Use 4 levels** for most classroom assessments (good discrimination without over-complexity)
- **Use 3 levels** for younger learners or simpler tasks
- **Avoid 2 levels** (binary; insufficient discrimination)
- **Avoid >5 levels** (hard to distinguish; inter-rater reliability suffers)

**Level Labels**:
- Descriptive: Exemplary, Proficient, Developing, Beginning
- Numeric: 4, 3, 2, 1
- Standards-based: Exceeds, Meets, Approaching, Below
- Avoid judgmental: Excellent, Terrible (use descriptive instead)

### Step 2: Identify Evaluation Criteria

#### 2.1 Derive Criteria from Learning Objectives
1. **Start with Objectives**
   - What does the objective require students to demonstrate?
   - What are the essential components of quality performance?

2. **Analyze the Task**
   - What skills or knowledge are being assessed?
   - What makes one response better than another?
   - What do experts attend to when evaluating this type of work?

3. **Brainstorm Criteria**
   - List all possible quality dimensions
   - Aim for 3-6 criteria (analytic rubric)
   - Ensure independence (criteria don't overlap)
   - Focus on substance over surface (content > formatting, unless formatting is a genuine objective)

**Example - Research Paper**:
- Thesis/Argument: Clear, defensible claim
- Evidence: Relevant, credible sources
- Analysis: Reasoning connecting evidence to thesis
- Organization: Logical structure and flow
- Conventions: Grammar, citation format

**Example - Science Lab Report**:
- Hypothesis: Testable, based on theory
- Procedure: Detailed, replicable
- Data: Accurate, organized, complete
- Analysis: Correct calculations, interpretation
- Conclusion: Supported by data, addresses hypothesis

#### 2.2 Make Criteria Observable and Measurable
**Convert vague criteria to specific, observable indicators**:

❌ **Vague**: "Creativity"
✅ **Specific**: "Offers original solution not discussed in class; makes novel connections between concepts"

❌ **Vague**: "Good organization"
✅ **Specific**: "Clear introduction, body paragraphs with topic sentences, logical transitions, effective conclusion"

❌ **Vague**: "Understanding"
✅ **Specific**: "Accurately explains concept using correct terminology; applies concept to novel scenario"

#### 2.3 Ensure Criteria Alignment
- Each criterion maps to a learning objective
- Criteria collectively cover all objectives
- Appropriate emphasis (weight criteria by importance)
- No "gotcha" criteria unrelated to objectives
- Accessible language (students understand what's expected)

### Step 3: Define Performance Levels

#### 3.1 Start with the Proficient/Target Level
1. **Describe "Meets Standard" Performance**
   - What does proficient work look like?
   - What are the essential features?
   - Use specific, observable descriptors
   - Align to the learning objective

**Example - Criterion: Thesis Statement**
```
Proficient (3): States a clear, defensible claim that addresses
the prompt and provides a roadmap for the argument. Claim is
specific (not vague or overly broad) and arguable (not statement of fact).
```

#### 3.2 Define Higher Performance Level(s)
1. **Describe "Exceeds Standard"**
   - What distinguishes exemplary from proficient?
   - Focus on sophistication, nuance, insight
   - Not just "more" but "better quality"

**Example - Criterion: Thesis Statement**
```
Exemplary (4): States a nuanced, sophisticated claim that addresses
the prompt and acknowledges complexity. Claim reveals insight beyond
surface-level observation and suggests a multi-faceted argument.
Roadmap is clear and compelling.
```

#### 3.3 Define Lower Performance Level(s)
1. **Describe "Approaching" and "Beginning" Levels**
   - What does developing work look like?
   - What errors or omissions occur?
   - Maintain respectful, descriptive language

**Example - Criterion: Thesis Statement**
```
Developing (2): States a claim that addresses the prompt but is
vague, overly broad, or too simplistic. May be a statement of fact
rather than arguable position. Roadmap unclear or missing.

Beginning (1): No clear claim present, or claim does not address
the prompt. May restate the question without taking a position.
```

#### 3.4 Use Parallel Language Across Levels
- Consistent structure across performance levels
- Same dimensions addressed at each level
- Easy to compare across levels
- Clear progression from low to high

**Parallel Structure Example**:
```
Criterion: Evidence Use

4: Integrates diverse, highly credible sources; evaluates source quality
3: Uses relevant, credible sources; appropriate integration
2: Uses some relevant sources; integration inconsistent or superficial
1: Sources missing, irrelevant, or not credible; poor integration
```

### Step 4: Assign Point Values

#### 4.1 Determine Total Points
- Based on task importance and complexity
- Align with course grading structure
- Sufficient granularity for differentiation
- Practical for calculation

**Common Point Structures**:
- 4-point scale: 4, 3, 2, 1 (simple, clear)
- Weighted 4-point: 8, 6, 4, 2 (doubles weight)
- Percentage-based: 100%, 75%, 50%, 25%
- Custom: Match specific rubric needs

#### 4.2 Weight Criteria by Importance
**Equal Weighting**:
- All criteria worth same points
- Use when all dimensions equally important

**Differential Weighting**:
- More important criteria worth more points
- Reflects emphasis in objectives
- Example: Content criteria > Mechanics criteria

**Weighting Example - Research Paper (100 points total)**:
```
Criterion                   | Weight | Points per Level (4,3,2,1)
----------------------------|--------|---------------------------
Thesis/Argument             | 30%    | 30, 22.5, 15, 7.5
Evidence                    | 30%    | 30, 22.5, 15, 7.5
Analysis                    | 25%    | 25, 18.75, 12.5, 6.25
Organization                | 10%    | 10, 7.5, 5, 2.5
Conventions (grammar, cite) | 5%     | 5, 3.75, 2.5, 1.25
----------------------------|--------|---------------------------
TOTAL                       | 100%   | 100 points
```

### Step 5: Write Clear Descriptors

#### 5.1 Use Specific, Observable Language
**Effective Descriptors**:
- Concrete, observable behaviors or products
- Specific examples when helpful
- Quantifiable when appropriate (e.g., "at least 3 sources")
- Free from jargon or ambiguity

**Descriptor Quality Check**:
- ❌ "Excellent analysis" → What makes it excellent?
- ✅ "Draws sophisticated connections between multiple concepts; explains causal relationships with supporting evidence"

- ❌ "Good writing" → What defines good?
- ✅ "Clear topic sentences; logical transitions; varied sentence structure; minimal grammatical errors"

#### 5.2 Avoid Common Descriptor Pitfalls
1. **Comparative Language Without Anchor**
   - ❌ "Better organization than level 2"
   - ✅ "Clear introduction, body paragraphs with transitions, focused conclusion"

2. **Evaluative Without Description**
   - ❌ "Impressive creativity"
   - ✅ "Proposes original solution; makes unexpected connections; takes intellectual risks"

3. **Vague Qualifiers**
   - ❌ "Some errors", "mostly accurate", "fairly well"
   - ✅ "1-2 minor errors", "accurate with one exception", "addresses 3 of 4 required components"

4. **Negative Framing** (use sparingly)
   - Limit "does not" statements
   - Focus on what IS present at each level
   - Exception: Beginning level may require describing absences

### Step 6: Create the Rubric

#### 6.1 Build Analytic Rubric Template
```
Criterion | Exemplary (4) | Proficient (3) | Developing (2) | Beginning (1)
----------|---------------|----------------|----------------|---------------
[Criterion 1]
[Weight: X pts]

[Descriptor 4] [Descriptor 3] [Descriptor 2] [Descriptor 1]

[Criterion 2]
[Weight: Y pts]

[Descriptor 4] [Descriptor 3] [Descriptor 2] [Descriptor 1]

...
----------|---------------|----------------|----------------|---------------
TOTAL POINTS: ___ / [Max points]
```

**Template**: `templates/analytic-rubric-template.md`

#### 6.2 Build Holistic Rubric Template
```
Score | Descriptor
------|---------------------------------------------------------------------
  4   | [Description of exemplary overall performance addressing all key
      | dimensions with sophistication and skill...]

  3   | [Description of proficient performance meeting all requirements
      | with competence...]

  2   | [Description of developing performance with gaps or inconsistencies...]

  1   | [Description of beginning performance with significant gaps or errors...]
```

#### 6.3 Add Supporting Elements
**Helpful Additions**:
- **Title**: Clear identification of assessment
- **Instructions**: How to use the rubric
- **Definitions**: Clarify technical terms if needed
- **Examples**: Anchor papers or sample work (optional)
- **Conversion Table**: If converting to letter grades
- **Comment Space**: For personalized feedback

### Step 7: Validate and Refine Rubric

#### 7.1 Self-Review Checklist
- [ ] All learning objectives represented in criteria?
- [ ] Criteria observable and measurable?
- [ ] Descriptors specific and clear?
- [ ] Parallel structure across performance levels?
- [ ] Appropriate number of levels (3-5)?
- [ ] Point values/weights aligned to importance?
- [ ] Language accessible to students?
- [ ] No overlapping or redundant criteria?
- [ ] Can distinguish between performance levels?

**Checklist**: `checklists/rubric-quality.md`

#### 7.2 Peer Review
- Share rubric with colleague
- Ask: Can they use it to score sample work consistently?
- Gather feedback on clarity and usability
- Refine based on insights

#### 7.3 Pilot with Sample Work
1. **Gather Sample Student Work** (current or prior)
2. **Score Using Rubric**
   - Note areas of uncertainty
   - Identify hard-to-score responses
   - Check if rubric discriminates quality levels
3. **Revise Rubric**
   - Clarify ambiguous descriptors
   - Add examples if helpful
   - Adjust levels if gaps or overlaps exist

#### 7.4 Check Inter-Rater Reliability
- Two scorers independently rate same work
- Compare scores for agreement
- Discuss discrepancies and refine rubric
- Aim for >80% agreement on exact score, >95% within one level

### Step 8: Share Rubric with Students

#### 8.1 Make Rubric Transparent
**When to Share**:
- **Before** the task: Use as learning tool
- During instruction: Reference criteria while teaching
- During work time: Students self-assess progress
- After submission: Use to provide feedback

**Benefits of Pre-Sharing**:
- Clarifies expectations
- Focuses learning and effort
- Supports self-assessment and peer feedback
- Reduces anxiety about "hidden criteria"

#### 8.2 Teach Students to Use Rubric
**Instructional Strategies**:
1. **Analyze the Rubric**
   - Review each criterion and level
   - Discuss what quality looks like
   - Clarify any confusing terms

2. **Practice with Examples**
   - Provide sample work at different levels
   - Students score using rubric
   - Discuss and justify ratings

3. **Self-Assessment**
   - Students evaluate their own draft work
   - Identify strengths and areas for improvement
   - Set goals based on rubric criteria

4. **Peer Feedback**
   - Structured peer review using rubric
   - Focus on specific, actionable feedback
   - Practice constructive communication

**Canvas Implementation**:
- Post rubric in module
- Embed in assignment description
- Link to exemplar work

### Step 9: Build Rubric in Canvas

#### 9.1 Create Rubric in Canvas
```python
canvas_create_rubric(
    course_id=12345,
    title="Research Paper Rubric",
    criteria=[
        {
            "description": "Thesis/Argument",
            "points": 30,
            "long_description": "Clear, defensible claim addressing the prompt",
            "ratings": [
                {
                    "description": "Exemplary",
                    "points": 30,
                    "long_description": "Nuanced, sophisticated claim acknowledging complexity..."
                },
                {
                    "description": "Proficient",
                    "points": 22.5,
                    "long_description": "Clear, defensible claim addressing prompt..."
                },
                {
                    "description": "Developing",
                    "points": 15,
                    "long_description": "Claim present but vague or overly broad..."
                },
                {
                    "description": "Beginning",
                    "points": 7.5,
                    "long_description": "No clear claim or does not address prompt..."
                }
            ]
        },
        {
            "description": "Evidence",
            "points": 30,
            # ... (similar structure)
        },
        # ... (additional criteria)
    ],
    free_form_criterion_comments=True  # Allow comments per criterion
)
```

#### 9.2 Associate Rubric with Assignment
- Link rubric to Canvas assignment
- Rubric appears during grading
- Scores auto-calculated
- Students see rubric with feedback

```python
# Rubric is associated when creating assignment or via Canvas UI
# canvas_create_assignment references rubric_id
```

### Step 10: Use Rubric for Grading and Feedback

#### 10.1 Apply Rubric Consistently
**Scoring Best Practices**:
1. **Read All Work First** (get sense of range)
2. **Score One Criterion at a Time** (all papers for Criterion 1, then all for Criterion 2)
3. **Use Rubric Exactly** (don't improvise new standards)
4. **Blind Scoring** (hide student names if possible)
5. **Take Breaks** (avoid scoring fatigue)
6. **Calibrate** (re-check early papers after scoring several)

#### 10.2 Provide Criterion-Level Comments
**Effective Rubric Feedback**:
- Check the rating
- Add specific comment explaining the score
- Note strengths (what worked well)
- Note improvements (what to work on next)
- Be specific, actionable, and encouraging

**Example Feedback**:
```
Criterion: Evidence (Score: 22.5/30 - Proficient)

Strengths: You used 5 credible sources and integrated them smoothly
into your argument. The peer-reviewed journal articles added authority.

Areas for Growth: Include more diverse source types (currently all
academic articles). Consider adding a primary source or data visualization
to strengthen your argument in paragraph 3.

Next Steps: For future papers, aim for source diversity and deeper
analysis of how each source supports your specific claim.
```

**Canvas Grading**:
```python
canvas_grade_with_rubric(
    course_id=12345,
    assignment_id=67890,
    user_id=111,
    rubric_assessment={
        "criterion_1": {
            "points": 22.5,
            "comments": "Strong thesis with minor clarity issues..."
        },
        "criterion_2": {
            "points": 30,
            "comments": "Excellent use of diverse, credible sources..."
        },
        # ...
    },
    text_comment="Overall strong paper. Focus on transitions for next draft."
)
```

#### 10.3 Analyze Rubric Data
**Use Rubric Results to Improve Instruction**:
- Which criteria do students score lowest?
- Common patterns of strength or weakness?
- Do rubric scores correlate with overall performance?
- Are criteria distinguishing performance as intended?

**Canvas Analytics**:
```python
# Review assignment analytics
canvas_get_assignment(
    course_id=12345,
    assignment_id=67890,
    include_submission=True
)

# Analyze score distributions per criterion
# Identify instructional opportunities
```

## Tools Needed

### Canvas MCP Tools
- `canvas_create_rubric` - Build scoring tool
- `canvas_get_assignment` - Review assessment
- `canvas_grade_with_rubric` - Apply rubric scoring
- `canvas_create_page` - Share rubric and exemplars
- `canvas_update_rubric` - Refine based on use

### Templates
- `templates/analytic-rubric-template.md` - Standard format
- `templates/holistic-rubric-template.md` - Overall score
- `templates/single-point-rubric-template.md` - Standards-based

### Checklists
- `checklists/rubric-quality.md` - Design validation
- `checklists/inter-rater-reliability.md` - Consistency check

## Quality Checks

### Rubric Design Quality
1. **Validity**: Criteria align to learning objectives?
2. **Clarity**: Descriptors specific and observable?
3. **Utility**: Useful for feedback and grading?
4. **Fairness**: Free from bias; accessible language?
5. **Discrimination**: Distinguishes performance levels effectively?
6. **Consistency**: Can be applied reliably across students/raters?

### Criteria Quality Indicators
- **Observable**: Concrete behaviors/products, not internal states
- **Measurable**: Can be assessed with evidence
- **Independent**: No overlap between criteria
- **Comprehensive**: Cover all important dimensions of quality
- **Appropriate Number**: 3-6 criteria (not too few, not too many)

### Descriptor Quality Indicators
- **Specific**: Concrete details, not vague generalities
- **Parallel**: Consistent structure across levels
- **Progressive**: Clear distinction between adjacent levels
- **Respectful**: Descriptive language, not judgmental
- **Student-Friendly**: Accessible vocabulary and phrasing

## Common Pitfalls

### ❌ Avoid These Mistakes

1. **Vague Criteria**
   - ❌ "Quality of work"
   - ✅ "Accuracy of data analysis; clarity of written explanation"

2. **Too Many Criteria** (decision paralysis)
   - ❌ 12 criteria for a short essay
   - ✅ 4-5 criteria covering essential dimensions

3. **Too Many Levels** (can't distinguish)
   - ❌ 10-point scale with fine gradations
   - ✅ 4-point scale with clear differences

4. **Count-Based Only** (misses quality)
   - ❌ "Uses 5 sources" (what if they're irrelevant?)
   - ✅ "Uses relevant, credible sources effectively integrated"

5. **Non-Parallel Descriptors**
   - ❌ Level 4 describes thesis; Level 3 describes evidence; Level 2 describes organization
   - ✅ All levels describe the same dimension (e.g., thesis quality)

6. **Negative Phrasing**
   - ❌ "Does not include evidence; lacks organization; poor grammar"
   - ✅ "Minimal evidence; basic organization; several grammatical errors"

7. **Hidden from Students**
   - ❌ Keep rubric secret until grading
   - ✅ Share rubric before assignment; use as teaching tool

## Example Complete Rubric

**Context**: High School Science Lab Report

```
LAB REPORT RUBRIC (Total: 50 points)

Criterion 1: Hypothesis (10 points)
-------------------------------------
Exemplary (10):
Hypothesis is testable, clearly stated, and based on scientific theory
or prior knowledge. Includes specific predictions with reasoning.
Variables (independent, dependent, controlled) explicitly identified.

Proficient (7.5):
Hypothesis is testable and clearly stated. Variables identified.
Connection to theory or prior knowledge present but could be more explicit.

Developing (5):
Hypothesis present but not fully testable (too vague or too broad).
Variables incompletely identified. Limited connection to theory.

Beginning (2.5):
Hypothesis missing, not testable, or does not address the research question.
Variables not identified. No connection to scientific concepts.

Criterion 2: Procedure (10 points)
-----------------------------------
Exemplary (10):
Procedure is detailed, sequential, and replicable. All materials listed
with specific quantities. Safety precautions noted. Could be followed
by another person to produce same results.

Proficient (7.5):
Procedure is clear and mostly complete. Materials listed. Could generally
be replicated with minor clarification needed.

Developing (5):
Procedure present but lacks important details. Steps out of order or
unclear. Materials incomplete. Difficult to replicate.

Beginning (2.5):
Procedure missing, extremely vague, or impossible to follow.
Materials not listed or incorrect.

Criterion 3: Data Collection & Presentation (15 points)
--------------------------------------------------------
Exemplary (15):
All data recorded accurately in organized table with labels and units.
Multiple trials conducted. Data displayed in appropriate graph with
title, labels, and units. Patterns clearly visible.

Proficient (11.25):
Data recorded accurately and organized. Table and graph present with
labels and units. Minor formatting issues. Patterns visible.

Developing (7.5):
Data present but organization unclear or incomplete. Table or graph
missing or poorly formatted. Units or labels missing. Patterns hard to identify.

Beginning (3.75):
Data missing, inaccurate, or extremely disorganized. No table or graph,
or completely inappropriate for data type.

Criterion 4: Analysis & Conclusion (15 points)
-----------------------------------------------
Exemplary (15):
Analysis uses data to support or refute hypothesis with specific evidence.
Explains results using scientific concepts. Identifies sources of error
and their impact. Suggests meaningful improvements or future research.

Proficient (11.25):
Analysis addresses hypothesis using data. Connects to scientific concepts.
Identifies sources of error. Suggestions for improvement present.

Developing (7.5):
Analysis incomplete or only partially supported by data. Limited connection
to concepts. Error sources vague or missing. Suggestions superficial.

Beginning (3.75):
Analysis missing or not based on collected data. No connection to concepts.
Does not address hypothesis or identify errors.
```

**Total Score: ____ / 50**

**Overall Feedback**:
[Space for holistic comments on strengths and areas for growth]

## Related Tasks
- **Foundation**: `create-learning-objectives.md` - Align criteria to objectives
- **Assessment**: `create-assessment.md` - Design tasks to score
- **Framework**: `apply-backward-design.md` - UbD Stage 2 rubrics
- **Implementation**: `canvas-course-build.md` - Build rubrics in Canvas

## References
- Brookhart, S.M. (2013). *How to Create and Use Rubrics for Formative Assessment and Grading*
- Arter, J., & McTighe, J. (2001). *Scoring Rubrics in the Classroom: Using Performance Criteria for Assessing and Improving Student Performance*
- Wiggins, G. (2012). "Seven Keys to Effective Feedback" (rubrics as feedback tools)
- Stiggins, R.J. (2001). *Student-Involved Classroom Assessment* (rubric co-construction)
- Panadero, E., & Jonsson, A. (2013). "The use of scoring rubrics for formative assessment purposes revisited"
